{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Feature Selection\n",
    "\n",
    "Feature selection is the process of choosing features (aka 'variables', 'attributes', 'predictors',\n",
    "'columns', 'independent variables') to include in our models. This is useful in situations where there\n",
    "are many variables to choose from--a problem known as the \"curse of dimensionality\". Including too \n",
    "many predictors when training could lead to overfitting. It can also lead to models that are computationally\n",
    "more efficient to train and predict because there is less input. \n",
    "\n",
    "There are many ways to manage this process. We could do it manually (as we have thus far this semester).\n",
    "We will cover common techniques in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "We will use the KDDCup Network Attack dataset from Labs 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load txt file\n",
    "names = pd.read_csv('data/kddcup.names', header=None, delimiter=':',skiprows=1)\n",
    "\n",
    "# make column 0 into a list\n",
    "name_list = names[0].tolist()\n",
    "\n",
    "# add the last column with type\n",
    "name_list.append('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netattacks = pd.read_csv('data/kddcup.data_10_percent_corrected', names=name_list, header=None, index_col=None)\n",
    "\n",
    "# use a 0 (normal) or 1 (malicious) to code bad traffic\n",
    "netattacks['label'] = np.where(netattacks['type'] == 'normal.', 0, 1)\n",
    "\n",
    "netattacks = netattacks.select_dtypes(include=np.number)\n",
    "\n",
    "# train-test split\n",
    "train, test = train_test_split(netattacks, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with all predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns not label\n",
    "pred_vars = list(netattacks.columns)\n",
    "\n",
    "# remove 'label' because it is what we are trying to predict\n",
    "pred_vars.remove('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also split the predictors and label column. This makes some of the \n",
    "later tasks a bit easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[pred_vars]\n",
    "train_y = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get importance from a classifier\n",
    "Here we will fit a Random Forest with all of the predictors. We can view the \n",
    "feature importance scores from these predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(train_X, train_y)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Best Features\n",
    "The `SelectFromModel()` function uses the feature importance scores shown above to get a subset. We will\n",
    "use the default cutoff threshold, but you can make this more or less permissive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "model = SelectFromModel(clf, prefit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataframe with reduced columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = model.get_support()\n",
    "X_reduced = train_X.iloc[:, support]\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with all columns\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also train a model with the data selected during feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with reduced set of columns\n",
    "rf_reduced = RandomForestClassifier()\n",
    "rf_reduced.fit(X_reduced, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation script has been modified slightly from prior weeks. It now contains two lists, one\n",
    "has the classifiers and the other has the different training sets. This is required because \n",
    "feature selection removes columns. The `sklearn` models require the data passed to the `predict` functions \n",
    "to contain the exact same columns as the data used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test[pred_vars]\n",
    "test_X_reduced = test_X.iloc[:, support]\n",
    "test_y = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use test data to get evaluation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of our models\n",
    "fitted = [rf, rf_reduced]\n",
    "\n",
    "# list of test sets for each\n",
    "test_sets = [test_X, test_X_reduced]\n",
    "\n",
    "# empty dataframe to store the results\n",
    "result_table = pd.DataFrame(columns=['classifier_name', 'fpr','tpr','auc', \n",
    "                                     'log_loss', 'clf_report'])\n",
    "\n",
    "for i in range(len(fitted)):\n",
    "    # select classifier and testing data\n",
    "    clf = fitted[i]\n",
    "    test_ = test_sets[i]\n",
    "\n",
    "    # print the name of the classifier\n",
    "    print(clf.__class__.__name__)\n",
    "    \n",
    "    # get predictions\n",
    "    yproba = clf.predict_proba(test_)\n",
    "    yclass = clf.predict(test_)\n",
    "    \n",
    "    # auc information\n",
    "    fpr, tpr, _ = metrics.roc_curve(test_y,  yproba[:,1])\n",
    "    auc = metrics.roc_auc_score(test_y, yproba[:,1])\n",
    "    \n",
    "    # log loss\n",
    "    log_loss = metrics.log_loss(test_y, yproba[:,1])\n",
    "    \n",
    "    # add some other stats based on confusion matrix\n",
    "    clf_report = metrics.classification_report(test_y, yclass, digits=5)\n",
    "    \n",
    "    # add the results to the dataframe\n",
    "    result_table = result_table.append({'classifier_name':clf.__class__.__name__,\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc,\n",
    "                                        'log_loss': log_loss,\n",
    "                                        'clf_report': clf_report}, ignore_index=True)\n",
    "#result_table.set_index('classifier_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_table.index:\n",
    "    print('\\n---- statistics for', result_table.loc[i, 'classifier_name'], \"----\\n\")\n",
    "    print(result_table.loc[i, 'clf_report'])\n",
    "    print(\"Model AUC:\", result_table.loc[i, 'auc'])\n",
    "    print(\"Model log loss:\", result_table.loc[i, 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,12))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(result_table.loc[i]['classifier_name'], result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Use a Decision Tree as the initial classifier (before the `SelectFromModel` cell). How many important feature are there?\n",
    "2. Try feature selection using the `SelectKBest()` method\n",
    "   [documented here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest). Re-run the training and evaluation process. Do the features chosen with this method work better or worse than using\n",
    "   all variables or those chosen with the `SelectFromModel()` method?\n",
    "\n",
    "# Extra\n",
    "The evaluation script is somewhat cumbersome, especially with two lists (one for models and one for test data subsets). Simplify the evaluation script and put it into a function. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa3e7337b8d36894b7a51014394139a0eba3728352dce4cced1005d81d5028b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
