{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Validation and Tuning\n",
    "In this lab, you will learn how to use cross validation to improve \n",
    "your model evaluation and to optimize the parameters for\n",
    "various types of classification algorithms.\n",
    "\n",
    "The technique to do this is called **cross-validation**.\n",
    "Until now, we have been using a train and test set to evaluate our models.\n",
    "This method has some limitations. First, since the train and test sets\n",
    "are typically chosen randomly, it is possible that either of these\n",
    "samples (or both) do not reflect the general data well. This could\n",
    "result in models that appear better or much worse than their actual\n",
    "performance on new data. In addition, it can lead to \"guessing\" which\n",
    "parameters can work, rather than *searching* various parameters in \n",
    "a systematic way. \n",
    "\n",
    "Cross validation (CV) works by taking your training data,\n",
    "breaking it into smaller, roughly equal size subsets.\n",
    "For example, in 5-fold CV, we divide the data into 5 subsets.\n",
    "For the first fold, we will train a model using four subsets and \n",
    "use the remaining subset for testing. Then, we will choose a different\n",
    "four subsets for training and the fifth set for testing. The result is that\n",
    "we will train and test five different models. \n",
    "\n",
    "First, import the necessary tools for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import (KFold, ShuffleSplit,\n",
    "                                     StratifiedKFold, \n",
    "                                     StratifiedShuffleSplit)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data for this lab comes from a study of deception in high-stakes situations. These observations \n",
    "were statements from individuals in the trials of high-profile murders. Statements from guilty\n",
    "parties are marked as `lies` and all others as `truth`. \n",
    "\n",
    "The language of these utterances has been quantified using the total number of words and the sentiment\n",
    "(positive or negative) of the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>title_name</th>\n",
       "      <th>verdict</th>\n",
       "      <th>title</th>\n",
       "      <th>name</th>\n",
       "      <th>condition</th>\n",
       "      <th>transcript</th>\n",
       "      <th>total_words</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>119</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>trial_truth_008.mp4</td>\n",
       "      <td>Defendant / Jodi Arias</td>\n",
       "      <td>Guilty                                       ...</td>\n",
       "      <td>Defendant</td>\n",
       "      <td>Jodi Arias</td>\n",
       "      <td>lie</td>\n",
       "      <td>She was fine, laughing about simple little thi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>54</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.843404</td>\n",
       "      <td>0.542237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-0.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>-0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.750000</td>\n",
       "      <td>0.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   audio_file  \\\n",
       "count                     120   \n",
       "unique                    119   \n",
       "top      trial_truth_008.mp4    \n",
       "freq                        2   \n",
       "mean                      NaN   \n",
       "std                       NaN   \n",
       "min                       NaN   \n",
       "25%                       NaN   \n",
       "50%                       NaN   \n",
       "75%                       NaN   \n",
       "max                       NaN   \n",
       "\n",
       "                                              title_name  \\\n",
       "count                                                120   \n",
       "unique                                                44   \n",
       "top      Defendant / Jodi Arias                            \n",
       "freq                                                  18   \n",
       "mean                                                 NaN   \n",
       "std                                                  NaN   \n",
       "min                                                  NaN   \n",
       "25%                                                  NaN   \n",
       "50%                                                  NaN   \n",
       "75%                                                  NaN   \n",
       "max                                                  NaN   \n",
       "\n",
       "                                                  verdict      title  \\\n",
       "count                                                 120        120   \n",
       "unique                                                  9         11   \n",
       "top      Guilty                                       ...  Defendant   \n",
       "freq                                                   54         70   \n",
       "mean                                                  NaN        NaN   \n",
       "std                                                   NaN        NaN   \n",
       "min                                                   NaN        NaN   \n",
       "25%                                                   NaN        NaN   \n",
       "50%                                                   NaN        NaN   \n",
       "75%                                                   NaN        NaN   \n",
       "max                                                   NaN        NaN   \n",
       "\n",
       "              name condition  \\\n",
       "count          120       120   \n",
       "unique          31         2   \n",
       "top     Jodi Arias       lie   \n",
       "freq            33        60   \n",
       "mean           NaN       NaN   \n",
       "std            NaN       NaN   \n",
       "min            NaN       NaN   \n",
       "25%            NaN       NaN   \n",
       "50%            NaN       NaN   \n",
       "75%            NaN       NaN   \n",
       "max            NaN       NaN   \n",
       "\n",
       "                                               transcript  total_words  \\\n",
       "count                                                 120   120.000000   \n",
       "unique                                                119          NaN   \n",
       "top     She was fine, laughing about simple little thi...          NaN   \n",
       "freq                                                    2          NaN   \n",
       "mean                                                  NaN    66.666667   \n",
       "std                                                   NaN    37.843404   \n",
       "min                                                   NaN     8.000000   \n",
       "25%                                                   NaN    42.750000   \n",
       "50%                                                   NaN    59.500000   \n",
       "75%                                                   NaN    82.750000   \n",
       "max                                                   NaN   221.000000   \n",
       "\n",
       "         sentiment  \n",
       "count   120.000000  \n",
       "unique         NaN  \n",
       "top            NaN  \n",
       "freq           NaN  \n",
       "mean      0.064516  \n",
       "std       0.542237  \n",
       "min      -0.984400  \n",
       "25%      -0.307100  \n",
       "50%       0.000000  \n",
       "75%       0.504500  \n",
       "max       0.978300  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_data = pd.read_csv('data/trial_data_language.csv')\n",
    "trial_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us specify our variables that we will use for prediction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vars = ['total_words', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cross-validation in training\n",
    "We will now use cross-validation to train a model. With cross validation,\n",
    "we can get the performance of a model with several iterations of training \n",
    "and testing.\n",
    "\n",
    "We will still hold a portion of the data out so that we can compare the\n",
    "results of cross-validation to the test data.\n",
    "\n",
    "## Create train and test sets\n",
    "For this lab, we will use an 80/20 split, and we will\n",
    "use stratified sampling. This will ensure proportional amounts\n",
    "of the values in the `class` column in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data['class'] = np.where(trial_data['condition'] == 'truth', 0, 1)\n",
    "np.random.seed(516)\n",
    "\n",
    "# create train and test\n",
    "train, test = train_test_split(trial_data, test_size=0.20, stratify=trial_data['condition'])\n",
    "print(\"Rows in train:\", len(train))\n",
    "print(\"Rows in test:\", len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "# https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 516)\n",
    "split = splitter.split(trial_data, groups=trial_data['name'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = trial_data.iloc[train_inds]\n",
    "test = trial_data.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model and view results\n",
    "Now, we will train a random forest classifier\n",
    "with the stratified shuffle split. This will take longer than\n",
    "regular training, since it will train 5 different models.\n",
    "We can have the models scored using a number of criteria.\n",
    "The output contains a dictionary with various figures for\n",
    "each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "rf_base = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf_base, train[pred_vars], train['class'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a single statistic over the models\n",
    "print(cv_rf['test_roc_auc'])\n",
    "print(\"mean model AUC\", np.mean(cv_rf['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_rf['test_neg_log_loss'])\n",
    "# note that this uses negative log loss (log loss * (-1)), meaning that \n",
    "# higher values (those closer to 0) are better.\n",
    "print(\"mean model log loss \", np.mean(cv_rf['test_neg_log_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "One major strength of cross-validation is the ability to\n",
    "search for different paramters in model specification to \n",
    "see which work best. Before, this was a very slow and manual\n",
    "process. \n",
    "\n",
    "You can get an overview of model tuning \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "First, we will specifiy some parameters for random forests that we would like to try.\n",
    "In this example, we will vary the criterion (Gini or entropy) and the maximum tree\n",
    "depth (5, 10, 15, or None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a large number of models to train, and may be slow. This dataset\n",
    "is small, so it will not be much of an issue here.\n",
    "\n",
    "The total number of models can be found by multiplying the number of parameter options\n",
    "to try and the number of folds. In this example:\n",
    "2 criterion * 4 tree depth * 5 folds = 40 models to train.\n",
    "\n",
    "We must also specify a scoring method to optimize. Using different\n",
    "scoring methods may yield very different model parameters, so this \n",
    "must be thought out in advance! Here, we are using the AUC as our\n",
    "optimization criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier()\n",
    "rf_tuned = GridSearchCV(rf_base, param_grid=params, cv=StratifiedShuffleSplit(n_splits), scoring='roc_auc')\n",
    "rf_tuned.fit(train[pred_vars], train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, you can view lots of details about the optimization process. The code below shows the results of each parameter combination that we specified, averaged over all of the folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the model test scores for each paramter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.cv_results_['mean_test_score'])\n",
    "print(rf_tuned.cv_results_['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the parameter settings that generated the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune a neural network\n",
    "We will now apply this same process to a neural network. Since\n",
    "these are very different from random forests under the hood,\n",
    "the parameters to search over will be specific to multi-layer perceptrons.\n",
    "\n",
    "Ignore the warnings for Convergence. You can use an import to solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nnet_base = MLPClassifier()\n",
    "\n",
    "params = {'hidden_layer_sizes': [(100,), (10,10), (5,5,5)], \n",
    "          'solver': ['adam', 'lbfgs', 'sgd']}\n",
    "# 9 different param combos, 5 cv = total of 45 models\n",
    "# try with differnt scoring methods\n",
    "# we get different results with accuracy & log_loss\n",
    "nnet_tuned = GridSearchCV(nnet_base, param_grid=params, cv=StratifiedShuffleSplit(n_splits), scoring='roc_auc')\n",
    "nnet_tuned.fit(train[pred_vars], train['class'])\n",
    "# nnet_tuned.get_params()\n",
    "print(nnet_tuned.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nnet_tuned.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nnet_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Next, we will compare the results of the two models, much like we did last time.\n",
    "We will use the same method as last week to plot the ROC curves for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = [rf_tuned, nnet_tuned]\n",
    "\n",
    "result_table = pd.DataFrame(columns=['classifier_name', 'fpr','tpr','auc', \n",
    "                                     'log_loss', 'clf_report'])\n",
    "\n",
    "for clf in fitted:\n",
    "    print(clf.estimator)\n",
    "    yproba = clf.predict_proba(test[pred_vars])\n",
    "    yclass = clf.predict(test[pred_vars])\n",
    "    \n",
    "    # auc information\n",
    "    fpr, tpr, _ = metrics.roc_curve(test['class'],  yproba[:,1])\n",
    "    auc = metrics.roc_auc_score(test['class'], yproba[:,1])\n",
    "    \n",
    "    # log loss\n",
    "    log_loss = metrics.log_loss(test['class'], yproba[:,1])\n",
    "    \n",
    "    # add some other stats based on confusion matrix\n",
    "    clf_report = metrics.classification_report(test['class'], yclass)\n",
    "    \n",
    "    \n",
    "    result_table = result_table.append({'classifier_name':str(clf.estimator),\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc,\n",
    "                                        'log_loss': log_loss,\n",
    "                                        'clf_report': clf_report}, ignore_index=True)\n",
    "    \n",
    "\n",
    "\n",
    "result_table.set_index('classifier_name', inplace=True)\n",
    "# print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_table.index:\n",
    "    print('\\n---- statistics for', i, \"----\\n\")\n",
    "    print(result_table.loc[i, 'clf_report'])\n",
    "    print(\"Model AUC:\", result_table.loc[i, 'auc'])\n",
    "    print(\"Model log loss:\", result_table.loc[i, 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,12))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Tune the parameters for two additional algorithms. They can be those which were used in last lab, or\n",
    "    you can try some other algorithm. A list of the models in `sklearn` is\n",
    "    [here](https://scikit-learn.org/stable/supervised_learning.html). \n",
    "    Use a grid search on at least two different parameters for each model. For example, in the random forest\n",
    "    built in the lab, we modified the `criterion` and `max_depth` parameters.\n",
    "    1. You should now have four models to compare. Which model performs best on out-of-sample (`test`)\n",
    "       data? Include precision, recall, accuracy, $F_1$, and ROC/AUC when deciding.\n",
    "\n",
    "\n",
    "## Optional\n",
    "\n",
    "2. Try using the grid search technique on the netattacks data from earlier labs. \n",
    "    1. Does model performance improve with the optimized parameters? \n",
    "    2. How long does model training take? (this could be a long time!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa3e7337b8d36894b7a51014394139a0eba3728352dce4cced1005d81d5028b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
