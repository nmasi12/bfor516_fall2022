{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Validation and Tuning\n",
    "In this lab, you will learn how to use cross validation to improve \n",
    "your model evaluation and to optimize the parameters for\n",
    "various types of classification algorithms.\n",
    "\n",
    "The technique to do this is called **cross-validation**.\n",
    "Until now, we have been using a train and test set to evaluate our models.\n",
    "This method has some limitations. First, since the train and test sets\n",
    "are typically chosen randomly, it is possible that either of these\n",
    "samples (or both) do not reflect the general data well. This could\n",
    "result in models that appear better or much worse than their actual\n",
    "performance on new data. In addition, it can lead to \"guessing\" which\n",
    "parameters can work, rather than *searching* various parameters in \n",
    "a systematic way. \n",
    "\n",
    "Cross validation (CV) works by taking your training data,\n",
    "breaking it into smaller, roughly equal size subsets.\n",
    "For example, in 5-fold CV, we divide the data into 5 subsets.\n",
    "For the first fold, we will train a model using four subsets and \n",
    "use the remaining subset for testing. Then, we will choose a different\n",
    "four subsets for training and the fifth set for testing. The result is that\n",
    "we will train and test five different models. \n",
    "\n",
    "First, import the necessary tools for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import (KFold, ShuffleSplit,\n",
    "                                     StratifiedKFold, \n",
    "                                     StratifiedShuffleSplit)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data for this lab comes from a study of deception in high-stakes situations. These observations \n",
    "were statements from individuals in the trials of high-profile murders. Statements from guilty\n",
    "parties are marked as `lies` and all others as `truth`. \n",
    "\n",
    "The language of these utterances has been quantified using the total number of words and the sentiment\n",
    "(positive or negative) of the utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = pd.read_csv('data/trial_data_language.csv')\n",
    "trial_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us specify our variables that we will use for prediction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vars = ['total_words', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cross-validation in training\n",
    "We will now use cross-validation to train a model. With cross validation,\n",
    "we can get the performance of a model with several iterations of training \n",
    "and testing.\n",
    "\n",
    "We will still hold a portion of the data out so that we can compare the\n",
    "results of cross-validation to the test data.\n",
    "\n",
    "## Create train and test sets\n",
    "For this lab, we will use an 80/20 split, and we will\n",
    "use stratified sampling. This will ensure proportional amounts\n",
    "of the values in the `class` column in the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data['class'] = np.where(trial_data['condition'] == 'truth', 0, 1)\n",
    "np.random.seed(516)\n",
    "\n",
    "# create train and test\n",
    "train, test = train_test_split(trial_data, test_size=0.20, stratify=trial_data['condition'])\n",
    "print(\"Rows in train:\", len(train))\n",
    "print(\"Rows in test:\", len(test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model and view results\n",
    "Now, we will train a random forest classifier\n",
    "with the stratified shuffle split. This will take longer than\n",
    "regular training, since it will train 5 different models.\n",
    "We can have the models scored using a number of criteria.\n",
    "The output contains a dictionary with various figures for\n",
    "each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "rf_base = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf_base, train[pred_vars], train['class'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a single statistic over the models\n",
    "print(cv_rf['test_roc_auc'])\n",
    "print(\"mean model AUC\", np.mean(cv_rf['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_rf['test_neg_log_loss'])\n",
    "# note that this uses negative log loss (log loss * (-1)), meaning that \n",
    "# higher values (those closer to 0) are better.\n",
    "print(\"mean model log loss \", np.mean(cv_rf['test_neg_log_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "One major strength of cross-validation is the ability to\n",
    "search for different paramters in model specification to \n",
    "see which work best. Before, this was a very slow and manual\n",
    "process. \n",
    "\n",
    "You can get an overview of model tuning \n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "First, we will specifiy some parameters for random forests that we would like to try.\n",
    "In this example, we will vary the criterion (Gini or entropy) and the maximum tree\n",
    "depth (5, 10, 15, or None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a large number of models to train, and may be slow. This dataset\n",
    "is small, so it will not be much of an issue here.\n",
    "\n",
    "The total number of models can be found by multiplying the number of parameter options\n",
    "to try and the number of folds. In this example:\n",
    "2 criterion * 4 tree depth * 5 folds = 40 models to train.\n",
    "\n",
    "We must also specify a scoring method to optimize. Using different\n",
    "scoring methods may yield very different model parameters, so this \n",
    "must be thought out in advance! Here, we are using the AUC as our\n",
    "optimization criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier()\n",
    "rf_tuned = GridSearchCV(rf_base, param_grid=params, cv=StratifiedShuffleSplit(n_splits), scoring='roc_auc')\n",
    "rf_tuned.fit(train[pred_vars], train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, you can view lots of details about the optimization process. The code below shows the results of each parameter combination that we specified, averaged over all of the folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the model test scores for each paramter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.cv_results_['mean_test_score'])\n",
    "print(rf_tuned.cv_results_['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the parameter settings that generated the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune a neural network\n",
    "We will now apply this same process to a neural network. Since\n",
    "these are very different from random forests under the hood,\n",
    "the parameters to search over will be specific to multi-layer perceptrons.\n",
    "\n",
    "Ignore the warnings for Convergence. You can use an import to solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nnet_base = MLPClassifier()\n",
    "\n",
    "params = {'hidden_layer_sizes': [(100,), (10,10), (5,5,5)], \n",
    "          'solver': ['adam', 'lbfgs', 'sgd']}\n",
    "# 9 different param combos, 5 cv = total of 45 models\n",
    "# try with differnt scoring methods\n",
    "# we get different results with accuracy & log_loss\n",
    "nnet_tuned = GridSearchCV(nnet_base, param_grid=params, cv=StratifiedShuffleSplit(n_splits), scoring='roc_auc')\n",
    "nnet_tuned.fit(train[pred_vars], train['class'])\n",
    "# nnet_tuned.get_params()\n",
    "print(nnet_tuned.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nnet_tuned.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nnet_tuned.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Next, we will compare the results of the two models, much like we did last time.\n",
    "We will use the same method as last week to plot the ROC curves for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = [rf_tuned, nnet_tuned]\n",
    "\n",
    "result_table = pd.DataFrame(columns=['classifier_name', 'fpr','tpr','auc', \n",
    "                                     'log_loss', 'clf_report'])\n",
    "\n",
    "for clf in fitted:\n",
    "    print(clf.estimator)\n",
    "    yproba = clf.predict_proba(test[pred_vars])\n",
    "    yclass = clf.predict(test[pred_vars])\n",
    "    \n",
    "    # auc information\n",
    "    fpr, tpr, _ = metrics.roc_curve(test['class'],  yproba[:,1])\n",
    "    auc = metrics.roc_auc_score(test['class'], yproba[:,1])\n",
    "    \n",
    "    # log loss\n",
    "    log_loss = metrics.log_loss(test['class'], yproba[:,1])\n",
    "    \n",
    "    # add some other stats based on confusion matrix\n",
    "    clf_report = metrics.classification_report(test['class'], yclass)\n",
    "    \n",
    "    \n",
    "    result_table = result_table.append({'classifier_name':str(clf.estimator),\n",
    "                                        'fpr':fpr, \n",
    "                                        'tpr':tpr, \n",
    "                                        'auc':auc,\n",
    "                                        'log_loss': log_loss,\n",
    "                                        'clf_report': clf_report}, ignore_index=True)\n",
    "    \n",
    "\n",
    "\n",
    "result_table.set_index('classifier_name', inplace=True)\n",
    "# print(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result_table.index:\n",
    "    print('\\n---- statistics for', i, \"----\\n\")\n",
    "    print(result_table.loc[i, 'clf_report'])\n",
    "    print(\"Model AUC:\", result_table.loc[i, 'auc'])\n",
    "    print(\"Model log loss:\", result_table.loc[i, 'log_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,12))\n",
    "\n",
    "for i in result_table.index:\n",
    "    plt.plot(result_table.loc[i]['fpr'], \n",
    "             result_table.loc[i]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
    "\n",
    "plt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Tune the parameters for two additional algorithms. They can be those which were used in last lab, or\n",
    "    you can try some other algorithm. A list of the models in `sklearn` is\n",
    "    [here](https://scikit-learn.org/stable/supervised_learning.html). \n",
    "    Use a grid search on at least two different parameters for each model. For example, in the random forest\n",
    "    built in the lab, we modified the `criterion` and `max_depth` parameters.\n",
    "    1. You should now have four models to compare. Which model performs best on out-of-sample (`test`)\n",
    "       data? Include precision, recall, accuracy, $F_1$, and ROC/AUC when deciding.\n",
    "\n",
    "\n",
    "## Optional\n",
    "\n",
    "2. Try using the grid search technique on the netattacks data from earlier labs. \n",
    "    1. Does model performance improve with the optimized parameters? \n",
    "    2. How long does model training take? (this could be a long time!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4cce46d6be9934fbd27f9ca0432556941ea5bdf741d4f4d64c6cd7f8dfa8fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
